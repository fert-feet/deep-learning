{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ecd3999-18b6-41ab-bdd6-611caea99d01",
   "metadata": {},
   "source": [
    "### 从零开始多 GPU 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f47352b-55d2-43ef-a7db-e05277d3c0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c433fd-009b-4030-aad2-482e70561a61",
   "metadata": {},
   "source": [
    "定义修改的 LeNet 和 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a7f84d3-a926-4091-8102-4617743094a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化模型参数\n",
    "scale = 0.01\n",
    "W1 = torch.randn(size=(20, 1, 3, 3)) * scale\n",
    "b1 = torch.zeros(20)\n",
    "W2 = torch.randn(size=(50, 20, 5, 5)) * scale\n",
    "b2 = torch.zeros(50)\n",
    "W3 = torch.randn(size=(800, 128)) * scale\n",
    "b3 = torch.zeros(128)\n",
    "W4 = torch.randn(size=(128, 10)) * scale\n",
    "b4 = torch.zeros(10)\n",
    "params = [W1, b1, W2, b2, W3, b3, W4, b4]\n",
    "\n",
    "# 定义模型\n",
    "def lenet(X, params):\n",
    "    h1_conv = F.conv2d(input=X, weight=params[0], bias=params[1])\n",
    "    h1_activation = F.relu(h1_conv)\n",
    "    h1 = F.avg_pool2d(input=h1_activation, kernel_size=(2, 2), stride=(2, 2))\n",
    "    h2_conv = F.conv2d(input=h1, weight=params[2], bias=params[3])\n",
    "    h2_activation = F.relu(h2_conv)\n",
    "    h2 = F.avg_pool2d(input=h2_activation, kernel_size=(2, 2), stride=(2, 2))\n",
    "    h2 = h2.reshape(h2.shape[0], -1)\n",
    "    h3_linear = torch.mm(h2, params[4]) + params[5]\n",
    "    h3 = F.relu(h3_linear)\n",
    "    y_hat = torch.mm(h3, params[6]) + params[7]\n",
    "    return y_hat\n",
    "\n",
    "# 交叉熵损失函数\n",
    "loss = nn.CrossEntropyLoss(reduction='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca330297-1796-4ddd-8385-b0f2b4af7658",
   "metadata": {},
   "source": [
    "#### 数据同步"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffb5213-25ed-45d8-8423-d9b8381af495",
   "metadata": {},
   "source": [
    "向 GPU 分发参数并附加梯度（开启梯度）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "106fa294-9c03-499b-84d0-67628ac1ccd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(params, device):\n",
    "    new_params = [p.to(device) for p in params]\n",
    "    for p in new_params:\n",
    "        p.requires_grad_()\n",
    "    return new_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de050bd-ca34-4f85-9767-2edfef238573",
   "metadata": {},
   "source": [
    "将所有梯度复制到 GPU0 上累计梯度，再将累加的梯度复制回 GPU 进行参数更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a52fae93-63f1-42b9-9196-ca9aed99d2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def allreduce(data): # data[gpu][grad]，data 的行代表 gpu，列是局部梯度\n",
    "    for i in range(1, len(data)):\n",
    "        data[0][:] += data[i].to(data[0].device) # gpu0 梯度加上其他 gpu 梯度（梯度累加）\n",
    "    for i in range(1, len(data)):\n",
    "        data[i][:] += data[0].to(data[i].device) # 累加的梯度复制到所有 gpu 上"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f45359-fabe-4e44-82a6-7abe5da690ca",
   "metadata": {},
   "source": [
    "#### 数据分发"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7008293-819f-470b-a260-45978e25c415",
   "metadata": {},
   "source": [
    "`split_batch` 函数将数据（包括特征和标签）分发到 不同 GPU 中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c25b9d6-28c5-4389-9ff6-0e14786ba70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_batch(X, y, devices):\n",
    "    assert X.shape[0] == y.shape[0]\n",
    "    return (nn.parallel.scatter(X, devices), \n",
    "            nn.parallel.scatter(y, devices))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595b2620-1396-4ebc-bf35-73be0a6a54c5",
   "metadata": {},
   "source": [
    "#### 训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9290ca7-05b8-41df-a6c0-e76e929673da",
   "metadata": {},
   "source": [
    "`train_batch` 是一个 batch 的训练，先分发数据，再在不同 gpu 计算 loss 和 grad，再相加后分发"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21994086-998e-4648-b3a2-8800ddc28e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(X, y, device_params, devices, lr): # device_params 是？\n",
    "    X_shards, y_shards = split_batch(X, y, devices)\n",
    "    \"\"\" 在不同 gpu 上分别计算 loss，ls 是列表 \"\"\"\n",
    "    ls = [loss(lenet(X_shard, device_W), y_shard).sum()\n",
    "          for X_shard, y_shard, device_W in zip(X_shards, y_shards, device_params)]\n",
    "    for l in ls:\n",
    "        l.backward()\n",
    "\n",
    "    \"\"\" grad 相加并复制分发 \"\"\"\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(device_params[0])):\n",
    "            allreduce([device_params[c][i].grad for c in range(len(devices))])\n",
    "\n",
    "    \"\"\" 在各自 gpu 更新参数 \"\"\"\n",
    "    for param in device_params:\n",
    "        d2l.sgd(param, lr, X.shape[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4367fb3e-75d0-4dbc-9d28-140e59920eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_gpus, batch_size, lr):\n",
    "    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
    "    devices = [d2l.try_gpu(i) for i in range(num_gpus)]\n",
    "    device_params = [get_params(params, device) for device in devices] #模型参数复制到多个 gpu\n",
    "    num_epochs = 10\n",
    "    animator = d2l.Animator('epoch', 'test acc', xlim=[1, num_epochs])\n",
    "    timer = d2l.Timer()\n",
    "    for epoch in range(num_epochs):\n",
    "        timer.start()\n",
    "        for X, y in train_iter:\n",
    "            train_batch(X, y, device_params, devices, lr)\n",
    "            torch.cuda.synchronize()\n",
    "        timer.stop()\n",
    "        animator.add(epoch + 1, (d2l.evaluate_accuracy_gpu(\n",
    "            lambda x: lenet(x, device_params[0]), test_iter, devices[0]),))\n",
    "    print(f'测试精度：{animator.Y[0][-1]:.2f}，{timer.avg():.1f}秒/轮，'\n",
    "      f'在{str(devices)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb85e714-f949-4980-8774-573478b96ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train(num_gpus=1, batch_size=256, lr=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304e5637-e151-4b0f-b35a-9c1e2087b523",
   "metadata": {},
   "source": [
    "### 简洁实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e506cdb-3037-4403-b0cb-f9ac4ef35606",
   "metadata": {},
   "source": [
    "使用 `nn.DataParallel(net, device_ids=devices)` 实现并行计算，实际上使用该函数将 net 转换后，使用原来的单GPU训练步骤就可以（不要忘记把数据放到 gpu 上）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc70f1bd-d22c-437b-8044-84f5ef4aba6c",
   "metadata": {},
   "source": [
    "`DataParallel` 会将输入数据分割成多个小批次，分别分配到不同的 GPU 上进行计算，然后将各个 GPU 的计算结果汇总"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ab5b90a-9050-41ac-9c9c-8355ef435c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = nn.DataParallel(lenet, device_ids=devices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
