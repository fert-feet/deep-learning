{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以序列模型为例构建序列（如机器翻译），序列的字符用其再词表中的索引的形式表示\n",
    "考虑 source sentence 和 target sentence\n",
    "\n",
    "在真实项目中，关键需要拿到词表大小，特征大小方可生成 word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 5]), torch.Size([2, 5, 8]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 2\n",
    "# src_len = torch.randint(2, 5, (batch_size,)) # randint(low, high, shape)\n",
    "# tgt_len = torch.randint(2, 5, (batch_size,))\n",
    "\n",
    "# 句子长度\n",
    "src_len = torch.tensor([2, 4]).to(torch.int32) # batch_size 个句子，每个句子的长度为张量中数字，这里{hard code} batch_size = 2\n",
    "tgt_len = torch.tensor([4, 3]).to(torch.int32)\n",
    "\n",
    "# 词表大小\n",
    "max_num_src_words = 8\n",
    "max_num_tgt_words = 8\n",
    "\n",
    "# 特征大小\n",
    "model_dim = 8\n",
    "\n",
    "# 序列最大长度\n",
    "max_src_seq_len = 5\n",
    "max_tgt_seq_len = 5\n",
    "\n",
    "# 句子索引序列生成\n",
    "src_seq = [F.pad(torch.randint(1, max_num_src_words, (L,)), (0, max_src_seq_len-L)) for L in src_len] # 词表长度为 8，生成两个句子，每个句子中的元素都是单词在词表中的索引，如 [4, 7] 就是长度为 2 的句子\n",
    "tgt_seq = [F.pad(torch.randint(1, max_num_tgt_words, (L,)), (0, max_tgt_seq_len-L)) for L in tgt_len] # pad 将长度补齐 (0, max_tgt_seq_len-L)代表左边不补齐，右边补齐代码中长度的 0\n",
    "src_seq = torch.stack(src_seq) # 变为 batch_size * max_src_seq_len 作为输入\n",
    "tgt_seq = torch.stack(tgt_seq) # stack(input, dim) 先扩张维度再合并\n",
    "\n",
    "# embedding，最终生成的 word embedding size = batch_size * seq_len(padding) * 词表大小 \n",
    "src_embedding_table = nn.Embedding(max_num_src_words+1, model_dim) # 这里 +1 是因为有 pad，咱们写死了序列长度，真实序列长度可能就是词表大小 + padding\n",
    "tgt_embedding_table = nn.Embedding(max_num_tgt_words+1, model_dim) # embedding_table 留给 pad\n",
    "src_embedding = src_embedding_table(src_seq)\n",
    "tgt_embedding = tgt_embedding_table(src_seq)\n",
    "\n",
    "src_seq.shape, src_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## position embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p_{pos, 2i} = \\sin(\\frac {pos} {10000^{2i/d}}), p_{pos, 2i + 1} = \\cos(\\frac {pos} {10000^{2i/d}})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
       "            1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
       "          [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
       "            9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
       "          [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
       "            9.9980e-01,  2.0000e-03,  1.0000e+00],\n",
       "          [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9995e-02,\n",
       "            9.9955e-01,  3.0000e-03,  1.0000e+00],\n",
       "          [-7.5680e-01, -6.5364e-01,  3.8942e-01,  9.2106e-01,  3.9989e-02,\n",
       "            9.9920e-01,  4.0000e-03,  9.9999e-01]],\n",
       " \n",
       "         [[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
       "            1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
       "          [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
       "            9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
       "          [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
       "            9.9980e-01,  2.0000e-03,  1.0000e+00],\n",
       "          [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9995e-02,\n",
       "            9.9955e-01,  3.0000e-03,  1.0000e+00],\n",
       "          [-7.5680e-01, -6.5364e-01,  3.8942e-01,  9.2106e-01,  3.9989e-02,\n",
       "            9.9920e-01,  4.0000e-03,  9.9999e-01]]]),\n",
       " tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
       "            1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
       "          [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
       "            9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
       "          [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
       "            9.9980e-01,  2.0000e-03,  1.0000e+00],\n",
       "          [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9995e-02,\n",
       "            9.9955e-01,  3.0000e-03,  1.0000e+00],\n",
       "          [-7.5680e-01, -6.5364e-01,  3.8942e-01,  9.2106e-01,  3.9989e-02,\n",
       "            9.9920e-01,  4.0000e-03,  9.9999e-01]],\n",
       " \n",
       "         [[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
       "            1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
       "          [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
       "            9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
       "          [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
       "            9.9980e-01,  2.0000e-03,  1.0000e+00],\n",
       "          [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9995e-02,\n",
       "            9.9955e-01,  3.0000e-03,  1.0000e+00],\n",
       "          [-7.5680e-01, -6.5364e-01,  3.8942e-01,  9.2106e-01,  3.9989e-02,\n",
       "            9.9920e-01,  4.0000e-03,  9.9999e-01]]]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 假设所有序列（source 和 target）最大长度为 5\n",
    "max_position_len = 5\n",
    "\n",
    "# 两个矩阵分别就是公式中三角函数里面的分子和分母矩阵\n",
    "pos_mat = torch.arange(max_position_len).reshape(-1, 1)\n",
    "i_mat = torch.pow(10000, torch.arange(0, 8, 2).reshape(1, -1) / model_dim)\n",
    "\n",
    "# tensor 方法构造 position embedding table\n",
    "pe_embedding_table = torch.zeros(max_position_len, model_dim)\n",
    "pe_embedding_table[:, 0::2] = torch.sin(pos_mat / i_mat) # 使用 broadcast 机制\n",
    "pe_embedding_table[:, 1::2] = torch.cos(pos_mat / i_mat)\n",
    "\n",
    "# 普通方法 构造 position embedding table\n",
    "# pe_embedding_table = torch.zeros(max_position_len, model_dim)\n",
    "# for pos in range(max_position_len):\n",
    "#     for i in range(0, model_dim, 2):\n",
    "#         pe_embedding_table[pos][i] = math.sin(pos / math.pow(10000, i / model_dim))\n",
    "#         pe_embedding_table[pos][i+1] = math.cos(pos / math.pow(10000, i / model_dim))\n",
    "\n",
    "# 构造 position embedding，可能还有相加的方法\n",
    "pe_embedding = nn.Embedding(max_position_len, model_dim)\n",
    "pe_embedding.weight = nn.Parameter(pe_embedding_table, requires_grad=False)\n",
    "\n",
    "src_pos = torch.stack([torch.arange(max_position_len) for _ in src_len]).to(torch.int32)\n",
    "tgt_pos = torch.stack([torch.arange(max_position_len) for _ in tgt_len]).to(torch.int32)\n",
    "src_pe_embedding = pe_embedding(src_pos)\n",
    "tgt_pe_embedding = pe_embedding(tgt_pos)\n",
    "\n",
    "src_pe_embedding, tgt_pe_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder masked self-attentiond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在笔记 [transformer](https://www.yuque.com/ky2fe/ssst9u/ssbmqr80yevif1tf) 中没有提到，encoder 中也是有 masked attention 的，只是这里的 masked 和 decoder 中的有不一样，encoder 的 masked 是将 source sequence 中增广的那些 <pad> 屏蔽掉"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于 QKV 的形状是 [batch_size, max_seq_len, model_dim]，则 $QK^T$ 的形状是 [batch_size, max_seq_len, max_seq_len]，masked_matrix 的形状也要一致，值为 1 或 -inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建句子的有效位置（没有增广的）\n",
    "valid_encoder_pos = torch.unsqueeze(torch.stack([F.pad(torch.ones(L), (0, max_src_seq_len-L)) for L in src_len]), 2)\n",
    "\n",
    "# 构造词元之间的关联性（真实存在的词元和 pad 关联性为 0）\n",
    "valid_encoder_pos_mat = torch.bmm(valid_encoder_pos, torch.transpose(valid_encoder_pos, 1, 2))\n",
    "invalid_encoder_pos_mat = 1 - valid_encoder_pos_mat\n",
    "\n",
    "# 将无用的位置设置为 True，代表需要 mask 的位置\n",
    "mask_encoder_self_attention = invalid_encoder_pos_mat.to(torch.bool)\n",
    "\n",
    "\n",
    "# 实际使用，score 就是 QK 的结果\n",
    "# score = torch.randn(batch_size, max_src_seq_len, max_src_seq_len)\n",
    "# masked_score = score.masked_fill(mask_encoder_self_attention, -1e9)\n",
    "# prob = F.softmax(masked_score, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder masked cross self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[False, False,  True,  True,  True],\n",
      "         [False, False,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True],\n",
      "         [ True,  True,  True,  True,  True]],\n",
      "\n",
      "        [[False, False, False, False,  True],\n",
      "         [False, False, False, False,  True],\n",
      "         [False, False, False, False,  True],\n",
      "         [False, False, False, False,  True],\n",
      "         [ True,  True,  True,  True,  True]]])\n"
     ]
    }
   ],
   "source": [
    "# 构建句子的有效位置（没有增广的）\n",
    "valid_encoder_pos = torch.unsqueeze(torch.stack([F.pad(torch.ones(L), (0, max_src_seq_len-L)) for L in src_len]), 2)\n",
    "valid_decoder_pos = torch.unsqueeze(torch.stack([F.pad(torch.ones(L), (0, max_tgt_seq_len-L)) for L in tgt_len]), 2)\n",
    "\n",
    "# 构造词元之间的关联性（真实存在的词元和 pad 关联性为 0）\n",
    "valid_cross_pos_mat = torch.bmm(valid_encoder_pos, torch.transpose(valid_encoder_pos, 1, 2))\n",
    "invalid_cross_pos_mat = 1 - valid_cross_pos_mat\n",
    "\n",
    "# 将无用的位置设置为 True，代表需要 mask 的位置\n",
    "mask_cross_self_attention = invalid_cross_pos_mat.to(torch.bool)\n",
    "print(mask_cross_self_attention)\n",
    "\n",
    "\n",
    "# 实际使用，score 就是 QK 的结果\n",
    "# score = torch.randn(batch_size, max_src_seq_len, max_src_seq_len)\n",
    "# masked_score = score.masked_fill(mask_encoder_self_attention, -1e9)\n",
    "# prob = F.softmax(masked_score, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Decoder masked multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t 时刻遮盖 t 时刻后面的信息，就是一个下三角矩阵，每次都使有效位 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False,  True,  True,  True,  True],\n",
       "         [False, False,  True,  True,  True],\n",
       "         [False, False, False,  True,  True],\n",
       "         [False, False, False, False,  True],\n",
       "         [ True,  True,  True,  True,  True]],\n",
       "\n",
       "        [[False,  True,  True,  True,  True],\n",
       "         [False, False,  True,  True,  True],\n",
       "         [False, False, False,  True,  True],\n",
       "         [ True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True]]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# F.pad(tensor, ((last dim), (2nd dim)))，以下函数代表在最后一个维度（列）pad，在倒数第二个维度（行）pad\n",
    "# 构建 self-attention 的掩码矩阵\n",
    "valid_decoder_tri_mat = torch.stack([F.pad(torch.tril(torch.ones(L, L)), (0, max_tgt_seq_len-L, 0, max_tgt_seq_len-L)) for L in tgt_len])\n",
    "invalid_decoder_tri_mat = (1 - valid_decoder_tri_mat).to(torch.bool)\n",
    "\n",
    "invalid_decoder_tri_mat\n",
    "\n",
    "\n",
    "# 使用方法\n",
    "# score = torch.randn(batch_size, max_tgt_seq_len, max_tgt_seq_len)\n",
    "# masked_score = score.masked_fill(invalid_decoder_tri_mat, -1e9)\n",
    "# prob = F.softmax(masked_score, -1)\n",
    "# prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scaled self attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, atten_mask):\n",
    "    score = torch.bmm(Q, torch.transpose(K, -2, -1)) / torch.sqrt(model_dim) # 使用 -1，-2 维交换以适应所有维度，有可能不止三维\n",
    "    masked_score = score.masked_fill(atten_mask, -1e9)\n",
    "    prob = F.softmax(masked_score, -1)\n",
    "    context = torch.bmm(prob, V)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
