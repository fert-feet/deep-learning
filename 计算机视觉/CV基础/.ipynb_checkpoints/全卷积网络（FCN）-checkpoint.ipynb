{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15645df1-14cf-43f5-8c07-95460a6ae5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c9d288-da68-4db7-9ba1-7f7bca75a80a",
   "metadata": {},
   "source": [
    "使用预训练的 `resnet18` 作为基准网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "612e1766-429e-41f6-95bc-21e1b2bb5a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_net = torchvision.models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7457ed9-2a8e-4d4a-8818-3fcc3091b150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sequential(\n",
       "   (0): BasicBlock(\n",
       "     (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace=True)\n",
       "     (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (downsample): Sequential(\n",
       "       (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "       (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     )\n",
       "   )\n",
       "   (1): BasicBlock(\n",
       "     (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace=True)\n",
       "     (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       " ),\n",
       " AdaptiveAvgPool2d(output_size=(1, 1)),\n",
       " Linear(in_features=512, out_features=1000, bias=True)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(pretrained_net.children())[-3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a023132-6f1e-49cc-a6dc-a218eb2570f4",
   "metadata": {},
   "source": [
    "将 resnet18 最后两层去掉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79dd6458-799f-4c38-95b6-9a9efa1cc2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(*list(pretrained_net.children())[:-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e27a137-7285-4bee-9490-fba5158cca5d",
   "metadata": {},
   "source": [
    "得到去掉卷积层后，320 x 480 的图片网络中输出是 10 x 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79f297ee-15c7-482f-98ef-c5fd763a6d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 10, 15])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand(size=(1, 3, 320, 480))\n",
    "Y = net(X)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69455428-6245-4fd5-b7eb-84d6f335c1ae",
   "metadata": {},
   "source": [
    "使用大小为 64 的卷积核，填充 16，步幅 32，最终上采样为原图大小 320 x 480，且先通过卷积层将通道数变为 Pascal VOC2012 数据集的类别数（21），这里填充是 16，是因为要让高宽成倍增大（32倍），就要让 $k(64) = 2p + s(32)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec02703f-b3a7-41ab-9100-966740e9248c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 21, 320, 480])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(512, 21, kernel_size=1)\n",
    "tconv = nn.ConvTranspose2d(21, 21, kernel_size=64, padding=16, stride=32)\n",
    "tconv(conv(Y)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b568c7-5558-4540-a1aa-ff9446387a93",
   "metadata": {},
   "source": [
    "在网络中添加层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c01e9fc3-f7db-416a-936d-e1706b3fa465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sequential(\n",
       "   (0): BasicBlock(\n",
       "     (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace=True)\n",
       "     (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (downsample): Sequential(\n",
       "       (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "       (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     )\n",
       "   )\n",
       "   (1): BasicBlock(\n",
       "     (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace=True)\n",
       "     (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   )\n",
       " ),\n",
       " Conv2d(512, 21, kernel_size=(1, 1), stride=(1, 1)),\n",
       " ConvTranspose2d(21, 21, kernel_size=(64, 64), stride=(32, 32), padding=(16, 16))]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.add_module('final_conv', conv)\n",
    "net.add_module('transpose_conv', tconv)\n",
    "list(net.children())[-3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28435cab-7386-4901-a7b7-7670d098db26",
   "metadata": {},
   "source": [
    "采用双线性插值的上采样算法初始化转置卷积层，以下是双线性插值初始化的转置卷积层权重，输入的参数是通道数和卷积核大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4a2c4da-9129-4ca8-916b-6f4c43965723",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bilinear_kernel(in_channels, out_channels, kernel_size):\n",
    "    factor = (kernel_size + 1) // 2\n",
    "    if kernel_size % 2 == 1:\n",
    "        center = factor - 1\n",
    "    else:\n",
    "        center = factor - 0.5\n",
    "    og = (torch.arange(kernel_size).reshape(-1, 1),\n",
    "          torch.arange(kernel_size).reshape(1, -1))\n",
    "    filt = (1 - torch.abs(og[0] - center) / factor) * \\\n",
    "           (1 - torch.abs(og[1] - center) / factor)\n",
    "    weight = torch.zeros((in_channels, out_channels,\n",
    "                          kernel_size, kernel_size))\n",
    "    weight[range(in_channels), range(out_channels), :, :] = filt\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376658b8-c770-410d-9a49-dfbe87686dfa",
   "metadata": {},
   "source": [
    "### 正式训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abf7955-aeb7-43ce-b762-a901aa281086",
   "metadata": {},
   "source": [
    "读取数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "caebc1a1-d201-44a4-b391-57529336481a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, crop_size = 32, (320, 480)\n",
    "train_iter, test_iter = None, None\n",
    "# train_iter, test_iter = torchvision.datasets.VOCSegmentation\n",
    "# train_iter, test_iter = d2l.load_data_voc(batch_size, crop_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "509ac8b1-436c-4978-9090-f2a3f6f1ec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(inputs, targets):\n",
    "    return F.cross_entropy(inputs, targets, reduction='none').mean(1).mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e0d07957-d056-46c6-b251-7028fdb78ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epochs, lr, wd, devices = 5, 0.001, 1e-3, d2l.try_all_gpus()\n",
    "# trainer = torch.optim.SGD(net.parameters(), lr=lr, weight_decay=wd)\n",
    "# d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9bf68172-3a6e-42a9-9db0-d710a03023aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(img):\n",
    "    X = test_iter.dataset.normalize_image(img).unsqueeze(0)\n",
    "    pred = net(X.to(devices[0])).argmax(dim=1)\n",
    "    return pred.reshape(pred.shape[1], pred.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "99e3e2b5-8e05-44d3-a411-c053b44e0b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label2image(pred):\n",
    "    colormap = torch.tensor(d2l.VOC_COLORMAP, device=devices[0])\n",
    "    X = pred.long()\n",
    "    return colormap[X, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3afae8-8058-40ea-8f4e-9cae6a2275a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# voc_dir = d2l.download_extract('voc2012', 'VOCdevkit/VOC2012')\n",
    "# test_images, test_labels = d2l.read_voc_images(voc_dir, False)\n",
    "# n, imgs = 4, []\n",
    "# for i in range(n):\n",
    "#     crop_rect = (0, 0, 320, 480)\n",
    "#     X = torchvision.transforms.functional.crop(test_images[i], *crop_rect)\n",
    "#     pred = label2image(predict(X))\n",
    "#     imgs += [X.permute(1,2,0), pred.cpu(),\n",
    "#              torchvision.transforms.functional.crop(\n",
    "#                  test_labels[i], *crop_rect).permute(1,2,0)]\n",
    "# d2l.show_images(imgs[::3] + imgs[1::3] + imgs[2::3], 3, n, scale=2);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
