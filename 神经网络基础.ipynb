{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a036d24-2cab-4aa0-8d99-fa37e6c58b0c",
   "metadata": {},
   "source": [
    "### 层和块\n",
    "\n",
    "回顾多层感知机，以下是简洁实现，`nn.Sequential()` 定义了一种特殊的 `Module`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50f0127a-58d0-42f1-8e3c-0d952bcaf7b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0041, -0.0812,  0.0519, -0.0582,  0.0513, -0.1538, -0.0394, -0.0293,\n",
       "          0.0650, -0.1670],\n",
       "        [ 0.0208, -0.0116,  0.0035, -0.2275, -0.0389, -0.3061,  0.1036, -0.0503,\n",
       "          0.0294, -0.1793]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "X = torch.rand(2, 20)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d17b096-2d0b-42c0-a4fa-61590dfae4e1",
   "metadata": {},
   "source": [
    "自定义多层感知机，`MLP` 类继承 `nn.Module`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ff06d8e-d7c2-4ca5-9156-a591e259c85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() # 调用父类的 init() 函数，继承父类的参数\n",
    "        self.hidden = nn.Linear(20, 256)\n",
    "        self.out = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.out(F.relu(self.hidden(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "814c2810-57d5-4a92-986b-581d73dc20b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0816,  0.1456, -0.0858,  0.2108,  0.1235, -0.0143,  0.1002, -0.0445,\n",
       "         -0.1516, -0.0812],\n",
       "        [ 0.0262,  0.0969, -0.1211,  0.1634,  0.2465,  0.0334,  0.1324, -0.0690,\n",
       "         -0.2328, -0.1449]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP()\n",
    "net(X) # forward 是 Module 自带的方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5327ab6e-0632-41f5-b560-976a2e4b877b",
   "metadata": {},
   "source": [
    "通过继承 `nn.Module` 并重写 `init(), forward()` 函数，可以做很多事情"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b499899d-6809-4b79-aa57-2158cf10a6aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2109,  0.1698, -0.1485,  0.0298,  0.0496,  0.0377, -0.1035, -0.2001,\n",
       "         -0.0377,  0.1846],\n",
       "        [-0.1345,  0.1248, -0.0747,  0.1307,  0.0918, -0.0191, -0.0886, -0.2544,\n",
       "          0.1496,  0.0375]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for block in args: # block 是传进的层次类\n",
    "            self._modules[block] = block # 以字典形式存储层次\n",
    "\n",
    "    def forward(self, X):\n",
    "        for block in self._modules.values():\n",
    "            X = block(X) # 比如第一层 block = nn.Linear(20, 256)，X 就作为第一层的输入，又是第二层输出，巧妙\n",
    "        return X\n",
    "\n",
    "net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17caf763-d085-4428-a391-ca9dc8c3a062",
   "metadata": {},
   "source": [
    "下面也是继承并重写 `nn.Module` 中函数的案例，并没有什么意义，只是说可以继承并做很多事"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04e6d3c7-e5a0-462a-9b5a-01d0cb7a197f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1029, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyFunction(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weight = torch.rand((20, 20), requires_grad=False)\n",
    "        self.liner = nn.Linear(20, 20)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.liner(X)\n",
    "        X = F.relu(torch.mm(X, self.weight) + 1)\n",
    "        X = self.liner(X)\n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()\n",
    "\n",
    "net = MyFunction()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a2b78c-462b-4f19-b23b-cfbcd188b356",
   "metadata": {},
   "source": [
    "可以各种嵌套使用，只需要保证层次之间的矩阵乘法能顺利进行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63402626-9d72-4f4b-bd7b-5a541ca0248a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.1178, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestNLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(20, 128), nn.ReLU(), \n",
    "                            nn.Linear(128, 10), nn.ReLU())\n",
    "        self.linear = nn.Linear(10, 20)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "my_net = nn.Sequential(NestNLP(), nn.Linear(20, 20), MyFunction())\n",
    "my_net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20d180d-56bf-4a20-9c22-3cfa904dd592",
   "metadata": {},
   "source": [
    "### 参数管理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc85fdd-73b6-4fb8-bf14-38951e6085e4",
   "metadata": {},
   "source": [
    "#### 获取参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eaea757-71d4-4e70-be86-747f5c7d1ec5",
   "metadata": {},
   "source": [
    "先看单隐藏层的多层感知机"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3075f39a-365b-4917-b85f-159d3be6e0ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3717],\n",
       "        [-0.3441]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(nn.Linear(4, 5), nn.ReLU(), nn.Linear(5, 1))\n",
    "X = torch.rand(2, 4)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "33e03eab-b046-4ca5-aa0b-630c39bcb114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[ 0.1132, -0.1537, -0.4112, -0.3942, -0.0061]])), ('bias', tensor([-0.2289]))])\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([[ 0.1132, -0.1537, -0.4112, -0.3942, -0.0061]], requires_grad=True)\n",
      "tensor([[ 0.1132, -0.1537, -0.4112, -0.3942, -0.0061]])\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(net[2].state_dict()) # state_dict 保存权重和偏差\n",
    "print(type(net[2].weight))\n",
    "print(net[2].weight)\n",
    "print(net[2].weight.data) # 用 data 访问数据，grad 访问梯度\n",
    "print(net[2].weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b99415a-6b7f-4db9-9b4d-f8ff20e2f0c5",
   "metadata": {},
   "source": [
    "用 `named_parameters` 拿出层次参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9314fe45-2419-433f-b5bc-7304ef6d7b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', torch.Size([5, 4])) ('bias', torch.Size([5]))\n",
      "('0.weight', torch.Size([5, 4])) ('0.bias', torch.Size([5])) ('2.weight', torch.Size([1, 5])) ('2.bias', torch.Size([1]))\n"
     ]
    }
   ],
   "source": [
    "print(*[(name, param.shape) for name, param in net[0].named_parameters()]) # *就是把循环里的东西都拿出来放一起\n",
    "print(*[(name, param.shape) for name, param in net.named_parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f07b9ec6-6bac-4392-9536-154da4eb1459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2192, -0.2636, -0.1713,  0.0926, -0.2413])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()['0.bias'].data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48d276c-15ba-4901-a6b5-1ea7f19e6fe7",
   "metadata": {},
   "source": [
    "从嵌套块收集参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b0a6ea2d-1e87-482c-802d-4183f1ea38f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2898, -0.3428, -0.7995,  0.6197],\n",
       "        [ 0.2898, -0.3428, -0.7995,  0.6197],\n",
       "        [ 0.2898, -0.3428, -0.7995,  0.6197],\n",
       "        [ 0.2898, -0.3428, -0.7995,  0.6197]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def block1():\n",
    "    net = nn.Sequential(nn.Linear(2, 6), nn.ReLU(), nn.Linear(6, 2))\n",
    "    return net\n",
    "\n",
    "def block2():\n",
    "    net = nn.Sequential()\n",
    "    for i in range(4):\n",
    "        net.add_module(f'block-{i}', block1()) # 循环 4 次，每次加入 1 个 block1（3层），一共 12 层，每两层名字相同\n",
    "    return net\n",
    "\n",
    "rgnet = nn.Sequential(block2(), nn.Linear(2, 4))\n",
    "X = torch.rand(4, 2)\n",
    "rgnet(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84751e1-a7b2-45dd-810a-e67e902c4e04",
   "metadata": {},
   "source": [
    "`rgnet` 有两层，第一层 `block2` 有 4 层 `block1`，`block1` 自身有 3 层，`rgnet` 第二层就是一个全连接层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bfa24c5f-b07e-4efd-91a2-cde9d449d8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (block-0): Sequential(\n",
      "      (0): Linear(in_features=2, out_features=6, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=6, out_features=2, bias=True)\n",
      "    )\n",
      "    (block-1): Sequential(\n",
      "      (0): Linear(in_features=2, out_features=6, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=6, out_features=2, bias=True)\n",
      "    )\n",
      "    (block-2): Sequential(\n",
      "      (0): Linear(in_features=2, out_features=6, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=6, out_features=2, bias=True)\n",
      "    )\n",
      "    (block-3): Sequential(\n",
      "      (0): Linear(in_features=2, out_features=6, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=6, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (1): Linear(in_features=2, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(rgnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac80cdf-3b2a-40c9-baf8-a6c076d53c25",
   "metadata": {},
   "source": [
    "#### 初始化参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b54b85-cea3-4bd5-8db0-55c3aae3e7a0",
   "metadata": {},
   "source": [
    "`nn.init` 就是自带的初始化方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "baabd26b-936d-45c9-b886-fb9b9d9683f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0008, -0.0077,  0.0060,  0.0084, -0.0048]), tensor(0.))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_normal(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, mean=0, std=0.01) # normal_ 指的是直接替换，而不是返回值，normal是返回，带下划线的函数同理\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "net.apply(init_normal) # apply 就是对 net 所有的 layer 都操作一遍初始化\n",
    "net[2].weight.data[0], net[2].bias.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8d3e61b2-c6d7-43dd-8acc-8dcc315d4785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight',\n",
       "              tensor([[1., 1., 1., 1.],\n",
       "                      [1., 1., 1., 1.],\n",
       "                      [1., 1., 1., 1.],\n",
       "                      [1., 1., 1., 1.],\n",
       "                      [1., 1., 1., 1.]])),\n",
       "             ('0.bias', tensor([0., 0., 0., 0., 0.])),\n",
       "             ('2.weight', tensor([[1., 1., 1., 1., 1.]])),\n",
       "             ('2.bias', tensor([0.]))])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_constant(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 1) # constant 就是固定值\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "net.apply(init_constant)\n",
    "net.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8190fa-5f01-4e07-9868-b802bee8b8db",
   "metadata": {},
   "source": [
    "`xavier` 初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9b9d31cd-3573-4fc0-9511-07e7d4de2a7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight',\n",
       "              tensor([[-0.6852, -0.5216, -0.4039, -0.2840],\n",
       "                      [ 0.4072, -0.4805, -0.2878, -0.5593],\n",
       "                      [ 0.5416,  0.2844,  0.1964, -0.7874],\n",
       "                      [ 0.1614, -0.5096,  0.1630, -0.8154],\n",
       "                      [-0.6718, -0.0660,  0.5819, -0.6630]])),\n",
       "             ('0.bias', tensor([0., 0., 0., 0., 0.])),\n",
       "             ('2.weight',\n",
       "              tensor([[ 0.0696, -0.5404, -0.4006, -0.8577,  0.6255]])),\n",
       "             ('2.bias', tensor([0.]))])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def xavier(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "net.apply(xavier)\n",
    "net.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4243988b-1514-44e0-92bd-17a0c519b00a",
   "metadata": {},
   "source": [
    "初始化参数也能自定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7a962c7f-ce06-4b78-ae8b-98f8aeaea27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init weight torch.Size([5, 4])\n",
      "Init weight torch.Size([1, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0000,  5.6333, -5.5193, -0.0000],\n",
       "        [-0.0000, -9.2136,  9.0162,  0.0000],\n",
       "        [-0.0000,  5.2770, -7.2623,  7.9502],\n",
       "        [-0.0000,  7.6082,  0.0000, -5.1386],\n",
       "        [-0.0000, -5.6072,  7.0920, -9.6170]], requires_grad=True)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        print(\"Init\", *[(name, param.shape) for name, param in m.named_parameters()][0]) # named_named_parameters()[0] 是 weight\n",
    "        nn.init.uniform_(m.weight, -10, 10)\n",
    "        m.weight.data *= m.weight.data.abs() >= 5 # 保留绝对值大于 5 的权重\n",
    "\n",
    "net.apply(my_init)\n",
    "net[0].weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dd0082-da10-4c01-b199-98255374c8bc",
   "metadata": {},
   "source": [
    "也可以直接更改指定层次的信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "517da416-e7a6-4d8c-bc7a-56d8e2becf23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[85.0000,  8.6333, -2.5193,  3.0000],\n",
       "        [ 3.0000, -6.2136, 12.0162,  3.0000],\n",
       "        [ 3.0000,  8.2770, -4.2623, 10.9502],\n",
       "        [ 3.0000, 10.6082,  3.0000, -2.1386],\n",
       "        [ 3.0000, -2.6072, 10.0920, -6.6170]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data[:] += 1\n",
    "net[0].weight.data[0, 0] = 85\n",
    "net[0].weight.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa245b7c-df49-43d9-a54c-aec08d8a4a4f",
   "metadata": {},
   "source": [
    "参数绑定，就是在初始化的时候指定一个共享层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "040d158e-14d9-4f71-8323-00ac3d25809e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0281],\n",
       "        [-0.0270],\n",
       "        [-0.0375],\n",
       "        [-0.0435],\n",
       "        [-0.0469],\n",
       "        [-0.0329],\n",
       "        [-0.0568],\n",
       "        [-0.0479]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shared = nn.Linear(8, 8)\n",
    "net = nn.Sequential(nn.Linear(2, 8), nn.ReLU(), shared, nn.ReLU(), shared, nn.ReLU(), nn.Linear(8, 1))\n",
    "print(net[2].weight.data == net[4].weight.data)\n",
    "X = torch.rand(8, 2)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04696fa-a939-432f-a0c7-850cfcb47958",
   "metadata": {},
   "source": [
    "### 自定义层"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62733a76-5e98-4fda-b9c5-8f0adb733c67",
   "metadata": {},
   "source": [
    "自定义一个没有任何参数的层，实际上不管是层次还是 `Sequential` 都是继承的 `nn.Module`，基本都需要重写 `init, forward` 两个函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d14e9e7-d6e7-4d3f-9d1c-4e581d474426",
   "metadata": {},
   "source": [
    "比如 `MyLayer` 可以作为一个层次加入 `Sequential` 中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2b5f01e3-bea8-4a28-aaae-fca7bc287922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2., -1.,  0.,  1.,  2.])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyLayer(nn.Module):\n",
    "    # def __init__(self): # python3 在不用定义参数的情况下可以不重写这个 init 函数\n",
    "    #     super().__init__()\n",
    "\n",
    "    def forward(self, X):\n",
    "        return X - X.mean()\n",
    "\n",
    "net = MyLayer()\n",
    "X = torch.FloatTensor([1, 2, 3, 4, 5])\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "80ddface-0b80-49ac-a131-102ce5e3ac02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0033, -0.2587, -1.1727, -0.1707],\n",
       "        [-1.6400,  3.1341,  1.6217, -1.6836]])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyLinear(nn.Module):\n",
    "    def __init__(self, in_unit, out_unit):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(in_unit, out_unit)) # randn 符合标准正态分布，rand 符合 0-1 均匀分布\n",
    "        self.bias = nn.Parameter(torch.randn(out_unit,)) # 生成向量而不是标量\n",
    "\n",
    "    def forward(self, X):\n",
    "        return torch.mm(X, self.weight.data) + self.bias.data\n",
    "\n",
    "net = nn.Sequential(MyLinear(2, 4), nn.ReLU(), nn.Linear(4, 1))\n",
    "net[0].weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa54aff-da85-4bdd-97b4-d6a080798688",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
