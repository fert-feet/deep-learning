{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a036d24-2cab-4aa0-8d99-fa37e6c58b0c",
   "metadata": {},
   "source": [
    "### 层和块\n",
    "\n",
    "回顾多层感知机，以下是简洁实现，`nn.Sequential()` 定义了一种特殊的 `Module`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50f0127a-58d0-42f1-8e3c-0d952bcaf7b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0748,  0.2424, -0.0090, -0.1285,  0.2409, -0.0181,  0.0524, -0.0808,\n",
       "          0.0484, -0.2322],\n",
       "        [ 0.0220,  0.3030, -0.0059, -0.0274,  0.0983,  0.0382,  0.1165, -0.0497,\n",
       "         -0.1206, -0.1737]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "X = torch.rand(2, 20)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d17b096-2d0b-42c0-a4fa-61590dfae4e1",
   "metadata": {},
   "source": [
    "自定义多层感知机，`MLP` 类继承 `nn.Module`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ff06d8e-d7c2-4ca5-9156-a591e259c85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() # 调用父类的 init() 函数，继承父类的参数\n",
    "        self.hidden = nn.Linear(20, 256)\n",
    "        self.out = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.out(F.relu(self.hidden(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "814c2810-57d5-4a92-986b-581d73dc20b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0450, -0.3381,  0.0402, -0.1064,  0.2222, -0.2932, -0.0245, -0.0086,\n",
       "         -0.2098, -0.2456],\n",
       "        [ 0.0506, -0.3827, -0.0396, -0.2066, -0.0025, -0.1894,  0.0189, -0.0350,\n",
       "         -0.0803, -0.1245]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP()\n",
    "net(X) # forward 是 Module 自带的方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5327ab6e-0632-41f5-b560-976a2e4b877b",
   "metadata": {},
   "source": [
    "通过继承 `nn.Module` 并重写 `init(), forward()` 函数，可以做很多事情"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b499899d-6809-4b79-aa57-2158cf10a6aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1542,  0.0193,  0.0948, -0.2359, -0.1331, -0.2414,  0.1139,  0.2610,\n",
       "          0.0996,  0.0805],\n",
       "        [ 0.0840, -0.0317,  0.0824, -0.1600, -0.1869, -0.1273,  0.0793,  0.1774,\n",
       "          0.0734,  0.0447]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for block in args: # block 是传进的层次类\n",
    "            self._modules[block] = block # 以字典形式存储层次\n",
    "\n",
    "    def forward(self, X):\n",
    "        for block in self._modules.values():\n",
    "            X = block(X) # 比如第一层 block = nn.Linear(20, 256)，X 就作为第一层的输入，又是第二层输出，巧妙\n",
    "        return X\n",
    "\n",
    "net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17caf763-d085-4428-a391-ca9dc8c3a062",
   "metadata": {},
   "source": [
    "下面也是继承并重写 `nn.Module` 中函数的案例，并没有什么意义，只是说可以继承并做很多事"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04e6d3c7-e5a0-462a-9b5a-01d0cb7a197f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3490, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyFunction(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weight = torch.rand((20, 20), requires_grad=False)\n",
    "        self.liner = nn.Linear(20, 20)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.liner(X)\n",
    "        X = F.relu(torch.mm(X, self.weight) + 1)\n",
    "        X = self.liner(X)\n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()\n",
    "\n",
    "net = MyFunction()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a2b78c-462b-4f19-b23b-cfbcd188b356",
   "metadata": {},
   "source": [
    "可以各种嵌套使用，只需要保证层次之间的矩阵乘法能顺利进行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63402626-9d72-4f4b-bd7b-5a541ca0248a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.1973, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestNLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(20, 128), nn.ReLU(), \n",
    "                            nn.Linear(128, 10), nn.ReLU())\n",
    "        self.linear = nn.Linear(10, 20)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "my_net = nn.Sequential(NestNLP(), nn.Linear(20, 20), MyFunction())\n",
    "my_net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20d180d-56bf-4a20-9c22-3cfa904dd592",
   "metadata": {},
   "source": [
    "### 参数管理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc85fdd-73b6-4fb8-bf14-38951e6085e4",
   "metadata": {},
   "source": [
    "#### 获取参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eaea757-71d4-4e70-be86-747f5c7d1ec5",
   "metadata": {},
   "source": [
    "先看单隐藏层的多层感知机"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3075f39a-365b-4917-b85f-159d3be6e0ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2057],\n",
       "        [0.1305]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(nn.Linear(4, 5), nn.ReLU(), nn.Linear(5, 1))\n",
    "X = torch.rand(2, 4)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33e03eab-b046-4ca5-aa0b-630c39bcb114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[ 0.0894,  0.2311,  0.4206,  0.1787, -0.3332]])), ('bias', tensor([0.2057]))])\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([[ 0.0894,  0.2311,  0.4206,  0.1787, -0.3332]], requires_grad=True)\n",
      "tensor([[ 0.0894,  0.2311,  0.4206,  0.1787, -0.3332]])\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(net[2].state_dict()) # state_dict 保存权重和偏差\n",
    "print(type(net[2].weight))\n",
    "print(net[2].weight)\n",
    "print(net[2].weight.data) # 用 data 访问数据，grad 访问梯度\n",
    "print(net[2].weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b99415a-6b7f-4db9-9b4d-f8ff20e2f0c5",
   "metadata": {},
   "source": [
    "用 `named_parameters` 拿出层次参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9314fe45-2419-433f-b5bc-7304ef6d7b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', torch.Size([5, 4])) ('bias', torch.Size([5]))\n",
      "('0.weight', torch.Size([5, 4])) ('0.bias', torch.Size([5])) ('2.weight', torch.Size([1, 5])) ('2.bias', torch.Size([1]))\n"
     ]
    }
   ],
   "source": [
    "print(*[(name, param.shape) for name, param in net[0].named_parameters()]) # *就是把循环里的东西都拿出来放一起\n",
    "print(*[(name, param.shape) for name, param in net.named_parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f07b9ec6-6bac-4392-9536-154da4eb1459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4250,  0.1294, -0.3036, -0.3581,  0.0195])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()['0.bias'].data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48d276c-15ba-4901-a6b5-1ea7f19e6fe7",
   "metadata": {},
   "source": [
    "从嵌套块收集参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0a6ea2d-1e87-482c-802d-4183f1ea38f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4634, -0.7390, -0.4538,  0.2143],\n",
       "        [ 0.4635, -0.7392, -0.4538,  0.2143],\n",
       "        [ 0.4637, -0.7393, -0.4539,  0.2144],\n",
       "        [ 0.4636, -0.7392, -0.4538,  0.2144]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def block1():\n",
    "    net = nn.Sequential(nn.Linear(2, 6), nn.ReLU(), nn.Linear(6, 2))\n",
    "    return net\n",
    "\n",
    "def block2():\n",
    "    net = nn.Sequential()\n",
    "    for i in range(4):\n",
    "        net.add_module(f'block-{i}', block1()) # 循环 4 次，每次加入 1 个 block1（3层），一共 12 层，每两层名字相同\n",
    "    return net\n",
    "\n",
    "rgnet = nn.Sequential(block2(), nn.Linear(2, 4))\n",
    "X = torch.rand(4, 2)\n",
    "rgnet(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84751e1-a7b2-45dd-810a-e67e902c4e04",
   "metadata": {},
   "source": [
    "`rgnet` 有两层，第一层 `block2` 有 4 层 `block1`，`block1` 自身有 3 层，`rgnet` 第二层就是一个全连接层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfa24c5f-b07e-4efd-91a2-cde9d449d8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (block-0): Sequential(\n",
      "      (0): Linear(in_features=2, out_features=6, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=6, out_features=2, bias=True)\n",
      "    )\n",
      "    (block-1): Sequential(\n",
      "      (0): Linear(in_features=2, out_features=6, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=6, out_features=2, bias=True)\n",
      "    )\n",
      "    (block-2): Sequential(\n",
      "      (0): Linear(in_features=2, out_features=6, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=6, out_features=2, bias=True)\n",
      "    )\n",
      "    (block-3): Sequential(\n",
      "      (0): Linear(in_features=2, out_features=6, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=6, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (1): Linear(in_features=2, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(rgnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac80cdf-3b2a-40c9-baf8-a6c076d53c25",
   "metadata": {},
   "source": [
    "#### 初始化参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b54b85-cea3-4bd5-8db0-55c3aae3e7a0",
   "metadata": {},
   "source": [
    "`nn.init` 就是自带的初始化方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "baabd26b-936d-45c9-b886-fb9b9d9683f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0017, -0.0126,  0.0022,  0.0103, -0.0021]), tensor(0.))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_normal(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, mean=0, std=0.01) # normal_ 指的是直接替换，而不是返回值，normal是返回，带下划线的函数同理\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "net.apply(init_normal) # apply 就是对 net 所有的 layer 都操作一遍初始化\n",
    "net[2].weight.data[0], net[2].bias.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d3e61b2-c6d7-43dd-8acc-8dcc315d4785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight',\n",
       "              tensor([[1., 1., 1., 1.],\n",
       "                      [1., 1., 1., 1.],\n",
       "                      [1., 1., 1., 1.],\n",
       "                      [1., 1., 1., 1.],\n",
       "                      [1., 1., 1., 1.]])),\n",
       "             ('0.bias', tensor([0., 0., 0., 0., 0.])),\n",
       "             ('2.weight', tensor([[1., 1., 1., 1., 1.]])),\n",
       "             ('2.bias', tensor([0.]))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_constant(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 1) # constant 就是固定值\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "net.apply(init_constant)\n",
    "net.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8190fa-5f01-4e07-9868-b802bee8b8db",
   "metadata": {},
   "source": [
    "`xavier` 初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b9d31cd-3573-4fc0-9511-07e7d4de2a7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight',\n",
       "              tensor([[ 0.2314, -0.1278, -0.0762, -0.4161],\n",
       "                      [ 0.5370, -0.4357,  0.7246,  0.3739],\n",
       "                      [ 0.7294, -0.4062,  0.0834,  0.0025],\n",
       "                      [-0.0445, -0.5716, -0.0810, -0.6010],\n",
       "                      [ 0.2431,  0.5741,  0.7020,  0.7073]])),\n",
       "             ('0.bias', tensor([0., 0., 0., 0., 0.])),\n",
       "             ('2.weight',\n",
       "              tensor([[ 0.5427, -0.7971, -0.5501, -0.5127, -0.2577]])),\n",
       "             ('2.bias', tensor([0.]))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def xavier(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "net.apply(xavier)\n",
    "net.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4243988b-1514-44e0-92bd-17a0c519b00a",
   "metadata": {},
   "source": [
    "初始化参数也能自定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a962c7f-ce06-4b78-ae8b-98f8aeaea27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init weight torch.Size([5, 4])\n",
      "Init weight torch.Size([1, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0000,  5.4795, -8.3915, -0.0000],\n",
       "        [ 0.0000, -0.0000,  0.0000, -9.6023],\n",
       "        [-5.4950,  7.8027,  7.3714,  9.9293],\n",
       "        [ 0.0000,  0.0000,  0.0000, -8.9807],\n",
       "        [ 8.5699, -6.0642, -7.7512, -0.0000]], requires_grad=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        print(\"Init\", *[(name, param.shape) for name, param in m.named_parameters()][0]) # named_named_parameters()[0] 是 weight\n",
    "        nn.init.uniform_(m.weight, -10, 10)\n",
    "        m.weight.data *= m.weight.data.abs() >= 5 # 保留绝对值大于 5 的权重\n",
    "\n",
    "net.apply(my_init)\n",
    "net[0].weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dd0082-da10-4c01-b199-98255374c8bc",
   "metadata": {},
   "source": [
    "也可以直接更改指定层次的信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "517da416-e7a6-4d8c-bc7a-56d8e2becf23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[85.0000,  6.4795, -7.3915,  1.0000],\n",
       "        [ 1.0000,  1.0000,  1.0000, -8.6023],\n",
       "        [-4.4950,  8.8027,  8.3714, 10.9293],\n",
       "        [ 1.0000,  1.0000,  1.0000, -7.9807],\n",
       "        [ 9.5699, -5.0642, -6.7512,  1.0000]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data[:] += 1\n",
    "net[0].weight.data[0, 0] = 85\n",
    "net[0].weight.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa245b7c-df49-43d9-a54c-aec08d8a4a4f",
   "metadata": {},
   "source": [
    "参数绑定，就是在初始化的时候指定一个共享层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "040d158e-14d9-4f71-8323-00ac3d25809e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0585],\n",
       "        [0.0930],\n",
       "        [0.0923],\n",
       "        [0.0575],\n",
       "        [0.0610],\n",
       "        [0.0961],\n",
       "        [0.0745],\n",
       "        [0.0693]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shared = nn.Linear(8, 8)\n",
    "net = nn.Sequential(nn.Linear(2, 8), nn.ReLU(), shared, nn.ReLU(), shared, nn.ReLU(), nn.Linear(8, 1))\n",
    "print(net[2].weight.data == net[4].weight.data)\n",
    "X = torch.rand(8, 2)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04696fa-a939-432f-a0c7-850cfcb47958",
   "metadata": {},
   "source": [
    "### 自定义层"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62733a76-5e98-4fda-b9c5-8f0adb733c67",
   "metadata": {},
   "source": [
    "自定义一个没有任何参数的层，实际上不管是层次还是 `Sequential` 都是继承的 `nn.Module`，基本都需要重写 `init, forward` 两个函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d14e9e7-d6e7-4d3f-9d1c-4e581d474426",
   "metadata": {},
   "source": [
    "比如 `MyLayer` 可以作为一个层次加入 `Sequential` 中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b5f01e3-bea8-4a28-aaae-fca7bc287922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2., -1.,  0.,  1.,  2.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyLayer(nn.Module):\n",
    "    # def __init__(self): # python3 在不用定义参数的情况下可以不重写这个 init 函数\n",
    "    #     super().__init__()\n",
    "\n",
    "    def forward(self, X):\n",
    "        return X - X.mean()\n",
    "\n",
    "net = MyLayer()\n",
    "X = torch.FloatTensor([1, 2, 3, 4, 5])\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80ddface-0b80-49ac-a131-102ce5e3ac02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1706, -0.4433, -0.1845,  0.1711],\n",
       "        [ 0.1412, -1.5087, -1.5302, -0.9381]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyLinear(nn.Module):\n",
    "    def __init__(self, in_unit, out_unit):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(in_unit, out_unit)) # randn 符合标准正态分布，rand 符合 0-1 均匀分布\n",
    "        self.bias = nn.Parameter(torch.randn(out_unit,)) # 生成向量而不是标量\n",
    "\n",
    "    def forward(self, X):\n",
    "        return torch.mm(X, self.weight.data) + self.bias.data\n",
    "\n",
    "net = nn.Sequential(MyLinear(2, 4), nn.ReLU(), nn.Linear(4, 1))\n",
    "net[0].weight.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad37dd11-aeea-4aaa-b649-a8947b61923d",
   "metadata": {},
   "source": [
    "### 读写文件"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46f22a2-1c42-4c7c-ab87-1d962fa846a7",
   "metadata": {},
   "source": [
    "加载和保存张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d22075b-3cfe-412b-890a-c066f64cc460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "x = torch.arange(4)\n",
    "torch.save(x, 'x-file')\n",
    "\n",
    "x2 = torch.load(\"x-file\")\n",
    "x2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6fbe9e-6217-437e-baf3-e33a89532525",
   "metadata": {},
   "source": [
    "存储张量列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef81872e-cd37-4b8c-88ad-1c38cef5b44d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3]), tensor([1., 1., 1., 1.]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.ones(4)\n",
    "torch.save([x, y], 'x-file')\n",
    "x2, y2 = torch.load(\"x-file\")\n",
    "x2, y2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc9eb47-f737-4055-9dee-6a6c8238cff9",
   "metadata": {},
   "source": [
    "可以用各种数据结构存储张量并存储"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b167a4c5-4247-4061-8e9c-f2a06ff3e37f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': tensor([0, 1, 2, 3]), 'y': tensor([1., 1., 1., 1.])}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_dict = {'x' : x, 'y' : y}\n",
    "torch.save(tensor_dict, 'x-file')\n",
    "torch.load(\"x-file\")\n",
    "tensor_dict_out = torch.load(\"x-file\")\n",
    "tensor_dict_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32205c47-7eed-4cd7-8d41-9cfe507c8768",
   "metadata": {},
   "source": [
    "加载和保存模型参数，`pytorch` 一般是存储参数，如权重和偏差，以字典形式存储，调用 `net.state_dict()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "832003cb-31b7-4ffa-a265-2516fb170fc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3066],\n",
       "        [-0.2063]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(2, 4)\n",
    "        self.output = nn.Linear(4, 1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.output(F.relu(self.hidden(X)))\n",
    "\n",
    "net = MLP()\n",
    "X = torch.randn(2, 2)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69c642f-d98a-4cb4-b6a4-58e8ba255a2a",
   "metadata": {},
   "source": [
    "命名方式就是以 `模型名称.params` 命名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cf5f475b-877a-46e6-8f1f-fe200f2fb572",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'mlp.params')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6117d15f-2e59-4869-94bb-b11aae6d8ca2",
   "metadata": {},
   "source": [
    "加载的时候要克隆一份模型（备份）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "77a2f433-9e03-4a6c-90c0-2904d6bcd149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (hidden): Linear(in_features=2, out_features=4, bias=True)\n",
       "  (output): Linear(in_features=4, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_clone = MLP()\n",
    "net_clone.load_state_dict(torch.load(\"mlp.params\"))\n",
    "net_clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fa35db66-d8b5-416b-b5f9-7a0ce14f49fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True],\n",
       "        [True]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_clone(X) == net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcfd307-3bc8-4d03-b4a6-e26f45819c8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
