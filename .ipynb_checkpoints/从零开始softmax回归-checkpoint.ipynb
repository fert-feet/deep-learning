{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3af0550-72b4-4380-8be5-87d7906dffdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from IPython import display\n",
    "from d2l import torch as d2l\n",
    "\n",
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef179ab-e417-4d9e-b6cb-ca448334d95a",
   "metadata": {},
   "source": [
    "### w，b 定义"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9958a1eb-ac1a-46c9-9401-b5347dc06552",
   "metadata": {},
   "source": [
    "展平图像，将其视为长度 784（28 * 28） 的向量，而标签是 10 个且作为输出（理所应当），由于输出是一个 `one-hot` 列表，\n",
    "则公式应为 $output_{n * 10} = <X_{n * 784}, W_{784 * 10}> + b_{n * 10}$，这里的 `n` 代表的其实是样本数（batch_size）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5052eb5-76ef-49eb-8d33-28e3117ce0d8",
   "metadata": {},
   "source": [
    "对于公式的解释是：$X$ 代表 一定样本数的数据，每个样本数由 784 个点位组成且 784 个点位作为一个整体，$XW$ 代表每个整体点位都和每个标签权重做一次运算得到分类结果，行数只代表样本，每一行代表不同样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38b2d35b-4abb-4f05-9933-b748850e60b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 784 # softmax 输入向量\n",
    "num_outputs = 10 # softmax 输出（10个标签 0-9）\n",
    "W = torch.normal(0, 0.01, size=(num_inputs, num_outputs), requires_grad=True) # 权重\n",
    "b = torch.zeros(num_outputs, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97af7c4e-b61a-4930-aa0f-cb2f8e298098",
   "metadata": {},
   "source": [
    "### 实现 softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4822fa-dc73-454d-b99d-e665d3d8996d",
   "metadata": {},
   "source": [
    "$$\n",
    "softmax(X)_{ij} = \\frac {exp(X_{ij})} {\\sum_k exp(X_{ik})}\n",
    "$$\n",
    "> $exp(X_{ij}) 代表对每一行进行 exp 操作$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cd253d8-d65a-4e7c-b4b4-51ac34d4b2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    X_exp = torch.exp(X)\n",
    "    partition = X_exp.sum(1, keepdim=True)\n",
    "    return X_exp / partition #广播机制，输出的每一行和为 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a567d098-de3b-4d94-9628-a4b43a0ca884",
   "metadata": {},
   "source": [
    "### 实现 softmax 回归模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b7fcff0-64d6-4b20-8bc3-41b71ed013e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    return softmax(torch.matmul(X.reshape(-1, W.shape[0]), W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcb9bd1-dc73-479b-91bf-e729fcf4e988",
   "metadata": {},
   "source": [
    "#### 补充：`y_hat` 说明"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b65fb8-5331-43f7-9691-14d6b43fddd1",
   "metadata": {},
   "source": [
    "这里索引代表，从 `y_hat` 两个维度中拿到和 `y` 中 0，1 下标对应数字的下标的值，即 `list(y_hat[0][y[0]], y_hat[1][y[1]])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f03f5fb-9ce5-4eda-8dc0-9e6a8b58d1da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.1000, 0.5000]), list[tensor(0.1000), tensor(0.5000)])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.tensor([0, 2])\n",
    "y_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.4, 0.5]])\n",
    "y_hat[[0, 1], y] , list[y_hat[0][y[0]], y_hat[1][y[1]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df64d15-ddaf-44f7-abcf-e313f246f107",
   "metadata": {},
   "source": [
    "### 实现交叉熵损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a045df-5e8f-409b-b114-457d00d1706e",
   "metadata": {},
   "source": [
    "公式:$\\ell(y,\\hat y) = \\sum_i{-y_ilog(y_i)} = -log{\\hat y_y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "234b0b3b-9fa6-44c2-8955-e0e8438d8b5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.3026, 0.6931])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cross_entropy(y_hat, y):\n",
    "    return -torch.log(y_hat[range(len(y_hat)), y]) # y_hat[range(len(y_hat)), y] 与 补充中的代码类似\n",
    "\n",
    "cross_entropy(y_hat, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1106bb3a-f138-45af-9cfe-dcdf76f411a7",
   "metadata": {},
   "source": [
    "将预测类别与真实 `y` 元素进行比较"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "391cb1c2-4f53-4b79-98d1-29a97bde9ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def accuracy(y_hat, y):\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "        y_hat = y_hat.argmax(axis=1) # 将每一行元素值最大的下标存到 y_hat 中\n",
    "    cmp = y_hat.type(y.dtype) == y #将 y_hat 和 y 的元素类型相等后再比较每行下标是否相等，相等则赋 1\n",
    "    return float(cmp.type(y.dtype).sum()) #把布尔转为 int\n",
    "\n",
    "accuracy(y_hat, y) / len(y) #正确率"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9cfaf2-4a59-40e1-9dfd-8c1edc9328d1",
   "metadata": {},
   "source": [
    "评估在任意模型的准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef2dae0c-28da-49a2-a207-1bfb4a3c42a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(net, data_iter):\n",
    "    if isinstance(net, torch.nn.Module): # torch.nn.Module 是所有神经网络的基类\n",
    "        net.eval() # 模型设置为评估模式\n",
    "    metric = Accumulator(2) #累加器\n",
    "    for X, y in data_iter:\n",
    "        metric.add(accuracy(net(X), y), y.numel()) # numel 返回张量中元素总数，分别存储于下标 0 和 1\n",
    "    return metric[0] / metric[1] # metric[0]表示累加器中正确预测的总数，metric[1]表示累加器中样本的总数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5f7c1a66-089a-4f65-8577-d30a624cc275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1256"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Accumulator:\n",
    "    def __init__(self, n): # 传入值是 n，也就是维度，有几个值要累加\n",
    "        self.data = [0.0] * n\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "evaluate_accuracy(net, test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bca457-e673-4d13-8227-5cc1a103e51e",
   "metadata": {},
   "source": [
    "### Softmax 回归的训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebcc687-1680-45b3-9d4d-639696fd0229",
   "metadata": {},
   "source": [
    "迭代函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "08273ee3-fd76-4c7c-b2f9-f0bb402d11b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_ch3(net, train_iter, loss, updater):\n",
    "    if isinstance(net, torch.nn.Module):\n",
    "        net.train() # 训练模式\n",
    "    metric = Accumulator(3)\n",
    "    for X, y in train_iter:\n",
    "        y_hat = net(X)\n",
    "        l = loss(y_hat, y)\n",
    "        if isinstance(updater, torch.optim.Optimizer): # 是否是 pytorch 自带的优化器，如果是就按照标准流程走\n",
    "            updater.zero_grad() #这里的 updater 是 pytorch 自带的 sgd\n",
    "            l.backward()\n",
    "            updater.step() # 梯度下降更新 w 和 b\n",
    "            metric.add(\n",
    "                float(l) * len(y), # 和下面的 float(l.sum()) 都代表总损失\n",
    "                accuracy(y_hat, y), \n",
    "                y.size().numel()\n",
    "            )\n",
    "        else:\n",
    "            l.sum().backward() #自定义优化函数一般要手动求和\n",
    "            updater(X.shape[0])\n",
    "            metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())\n",
    "    return metric[0] / metric[2], metric[1] / metric[2] # [0] 是总损失，[1] 是正确个数，[2] 是样本总数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5db560-8891-4e12-9da2-b50868d4b07e",
   "metadata": {},
   "source": [
    "主训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68c0216-13f9-409d-aff5-b37142457d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ch3(net, train_iter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
